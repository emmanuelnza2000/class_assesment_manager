groups:
  - name: classroom-manager.rules
    rules:
      - alert: PrometheusDown
        expr: up{job="prometheus"} == 0
        for: 2m
        labels:
          severity: critical
        annotations:
          summary: "Prometheus appears down"
          description: "Prometheus scrape target 'prometheus' is down for > 2m."

      - alert: HighContainerRestarts
        expr: increase(kube_pod_container_status_restarts_total[15m]) > 5
        for: 5m
        labels:
          severity: warning
        annotations:
          summary: "Container restarts high"
          description: "More than 5 restarts in 15m across pods."

      - alert: High5xxErrorRate
        expr: rate(http_requests_total{code=~"5.."}[5m]) > 0
        for: 5m
        labels:
          severity: critical
        annotations:
          summary: "High 5xx error rate"
          description: "Elevated 5xx error rate detected (requires app instrumentation)."

      # CPU-based scaling alert (example)
      - alert: HighCPUUsage
        expr: avg by (deployment) (rate(container_cpu_usage_seconds_total[2m])) > 0.5
        for: 2m
        labels:
          severity: warning
        annotations:
          summary: "High CPU usage on deployment {{ $labels.deployment }}"
          description: "Average CPU usage for deployment {{ $labels.deployment }} is high (>50% cpu cores averaged) for 2m. Consider scaling."

      # Example error-budget burn alert (placeholder, requires SLO recording rules)
      - alert: ErrorBudgetBurn
        expr: (sum(rate(http_requests_total{code!~"5.."}[30d])) / sum(rate(http_requests_total[30d]))) < 0.999
        for: 1h
        labels:
          severity: critical
        annotations:
          summary: "Error budget is burning"
          description: "Service is using error budget fast; investigate and consider rolling back or scaling."
